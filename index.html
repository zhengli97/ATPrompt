<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>ATPrompt</title>
  <link rel="icon" type="image/x-icon" href="none">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ATPrompt: Textual Prompt Learning with Embedded Attributes</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <p><a href="https://zhengli97.github.io/">Zheng Li</a><sup>1</sup>, <a href="https://scholar.google.com/citations?hl=en&user=oRhJHmIAAAAJ">Yibing Song</a><sup>2</sup>, <a href="https://scholar.google.com/citations?user=iN7TzlcAAAAJ&hl=en">Penghai Zhao</a><sup>1</sup>, <a href="https://scholar.google.com/citations?user=huWpVyEAAAAJ&hl=en">Ming-Ming Cheng</a><sup>1</sup>, <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ&hl=en">Xiang Li</a> <sup>1*</sup>, <a href="https://scholar.google.com/citations?user=6CIDtZQAAAAJ&hl=en">Jian Yang</a><sup>1*</sup></p>
              </span>
              </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"> <sup>1</sup> VCIP, College of Computer Science, Nankai University,
                       <br> <sup>2</sup>DAMO Academy, Alibaba Group
                    <br>
                    <span class="author-block"><strong>Arxiv 2024</strong></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                    <br>
                    <span class="author-block"><small>zhengli97[at]mail.nankai.edu.com</small></span>
                  </div>
                  <!-- <br> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <!-- <i class="fas fa-file-pdf"></i> -->
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>ArXiv(TBD)</span>
                      </a>
                    </span>
                  
                  <!-- ArXiv abstract Link -->
                  <!-- <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                    </span>
                    <span>ArXiv</span>
                  </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zhengli97/ATPrompt" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                  <!-- 英文解读 -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-clone"></i>
                    </span>
                    <span>Paper Interpretation(TBD)</span>

                  <!-- 中文解读 -->
                  <span class="link-block">
                    <a href="chinese_interpertation.html" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-clone"></i>
                    </span>
                    <span>中文解读(TBD)</span>
                </a>
                <br>
                <br>
                <figure>
                  <img src="static/images/demo.png" alt="fail" width="100%"">
                  <figcaption class="content has-text-centered"  style="font-size:0.9em">
                    <i>Table 1. Additional attribute information can facilitate the recognition of unknown categories.</i> 
                  </figcaption>
                </figure>
                <br>
                <div class="columns is-centered has-text-centered">
                  <div class="column is-five-fourths width:100%" >
                    <!-- <h2 class="title is 2">When people are faced with something in an unknown category, they often associate it with detailed attributes to increase clarity and understandability.</h2> -->
                    <p style="font-size: 14pt;">
                      When people are faced with something of an unknown category, they often recognize it by combining relevant detailed attributes (e.g., color, shape) to increase clarity and comprehensibility.
                    </p>
                    <br>
                    <p style="font-size: 14pt;">
                      Attributes can serve as <strong>bridges</strong> that connect unknown categories to our known knowledge.
                    </p>
                    <br>
                    <p style="font-size: 14pt;">
                      Inspired by this principle, can we let soft prompts learn attribute-related representations to improve the model's generalization ability?
                    </p>
                    <br>
                    <p style="font-size: 18pt;">
                      <strong>Of Course!</strong> 😎
                    </p>
                </div>
                </div>

                <h2 class="title is 2">Framework</h2>
                <figure>
                  <img src="static/images/compare.png" alt="fail" width="90%"">
                  <figcaption class="content has-text-centered"  style="font-size:0.9em;">
                    <i>Table 2. Architectural comparison between classic prompt learning  and our proposed attribute-embedded prompt learning. </i>
                  </figcaption>
                </figure>
                <br>
                <div class="columns is-centered has-text-centered">
                  <div class="column is-five-fourths width:100%" >
                    <p style="font-size: 14pt;">
                      We present <strong>ATPrompt</strong>, an attribute-embedded prompt learning method for VLMs.
                    </p>
                    <br>
                    <p style="font-size: 14pt;">
                      In this work, we propose to embeds multiple fixed attribute tokens into the set of soft tokens, <br/> transforming the original form Tab.2(a)
                      into an attribute-class mixed form Tab.2(b) for prompt learning.
                    </p>
                  </div>
                </div>
                <br>
                <figure>
                  <img src="static/images/deep_version.png" alt="fail" width="90%"">
                    <figcaption class="content has-text-left"  style="font-size:0.9em;">
                      <i>Table 3. An illustration of the computation process for shallow and deep versions. (a) The shallow version concatenates hard attribute tokens, 
                        soft prompt tokens, and class tokens and inputs them into the encoder for calculation.
                        (b) The deep version uses the same input but discards the class-related soft prompt tokens after calculating the self-attention and introduces
                        them again before the next layer.</i> 
                    </figcaption>
                </figure>
                <br>
                <h2 class="title is 2">Principle</h2>
                <figure>
                <img src="static/images/space.png" alt="fail" width="55%"">
                  <figcaption class="content has-text-left"  style="font-size:0.9em;">
                    <i>Table 4. Comparison of image and text (category) alignment processes through learnable prompts. (a) Current prompt learning
                      methods align images with predefined categories but fail to establish accurate associations with unknown categories. (b) ATPrompt
                      leverages universal attributes as an intermediary to create more accurate alignments between images and unknown categories.</i>
                  </figcaption>
                </figure>
                <br>
                <p style="font-size: 14pt;" class="content has-text-left">
                  By embedding multiple fixed universal attribute
                  tokens into the learnable soft prompts, our method extends the learning space of soft prompts from the original
                  one-dimensional category level (Tab.3(a)) to the multi-dimensional attribute level (Tab.3(b)).
                  <br><br>
                  Guided by these attributes, soft prompts acquire not only category-specific but
                  also attribute-related general representations during training, thereby enhancing the alignment between images and
                  unknown categories compared to the original method.
                </p>

                <div class="column is-five-fourths width:100%" >
                  <h2 class="title is 2">Q: How to determine attribute?</h2>
                  <h2 class="title is 2">A: Differentiable Attribute Search 🔎 !</h2>
                  <figure>
                    <img src="static/images/attribute_search.png" alt="fail" width="100%"">
                      <figcaption class="content has-text-centered"  style="font-size:0.9em; word-break:normal">
                          <i> Table 5. An overview of our attribute search pipeline.</i>
                      </figcaption>
                  </figure>
                  <br>
                  <p class="content has-text-left" style="font-size: 14pt;">
                    <strong>Step 1: Form an attribute pool.</strong>  We first query the LLM step by step to obtain multiple independent attributes, then combining them to create an attribute pool for subsequent search methods. 
                    <br>
                    <br>
                    <strong>Step 2: Differentiable attribute search.</strong> For each attribute base in the pool, we propose the use of an alternating algorithm to jointly optimize soft tokens and path weights. After training, the attribute with the highest weight (confidence) is selected for targeted model training.
                  </p>
                </div>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is 2">Contributions</h2>
        <div class="content has-text-left" style="word-break:normal" >
          <p style="font-size: 12pt;">
            <p>(1). We introduce an attribute-templated prompt learning method that expands the learning space of soft prompts from the original one-dimensional category level into the multi-dimensional attribute level by incorporating multiple universal attribute tokens into soft prompts.</p>
            <p>(2). We introduce a differentiable attribute search method that learns to determine the appropriate attribute content and quantity for the dataset.</p>
            <p>(3). Both shallow and deep versions of ATprompt are introduced to achieve compatibility with existing methods.</p>
            <p>(4). ATPrompt can be seamlessly intergrated into existing textual-based methods and brings general improvement at a negligible computational cost.</p>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="section hero is-light"> -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" style="text-indent:1em">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Textual-based prompt learning methods primarily employ multiple learnable soft prompts and hard class tokens in 
            a cascading manner as text prompt inputs, 
            aiming to align image and text (category) spaces for downstream tasks. However, current training is restricted to aligning 
            images with predefined known categories and cannot be associated with unknown categories. 
          </p>
          <p>
            In this work, we propose utilizing universal attributes as a bridge to enhance the alignment between images and unknown categories. 
            Specifically, we introduce an Attribute-embedded Textual Prompt learning method for vision-language models, named ATPrompt. 
            This approach expands the learning space of soft prompts 
            from the original one-dimensional category level into the multi-dimensional attribute level by incorporating multiple 
            universal attribute tokens into the learnable soft prompts. Through this modification, we transform the text prompt 
            from a category-centric form to an attribute-category hybrid form. 
          </p>
          <p>
            To finalize the attributes for downstream tasks, we propose a differentiable attribute search method that learns to identify 
            representative and suitable attributes from a candidate pool summarized by a large language model. 
          </p>
          <p>
            As an easy-to-use plug-in technique, ATPrompt can 
            seamlessly replace the existing prompt format of textual-based methods, offering general improvements at 
            a negligible computational cost. Extensive experiments on 11 datasets demonstrate the effectiveness of our method. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" >
        <h2 class="title is 5">Highlights</h2>
        <div class="content has-text-justified" style="word-break:normal" >
          <p>
            (1). A novel two-stage unsupervised prompt distillation framework for Vision-Language Models.<p>
            (2). Reuse high-quality teacher text features instead of training the student's own text encoder. <p>
            (3). Distillation on large amounts of unlabeled domain images using soft labels provided by teacher.<p>
            (4). PromptKD outperforms all existing prompt learning methods on 11 diverse recognition datasets.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- 
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" style="width:100%">
        <h2 class="title is 5">Calculation Pipeline</h2>
        <div class="content has-text-left no-auto-width" style="word-break:normal" >
          <figure>
            <img src="static/images/pipeline.png" alt="fail" width="50%"">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>x -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">A Quick Overview of Experimental Results</h2>
        <div class="content has-text-left is-size-5">
        <strong>Base-to-Novel Generalization</strong>
        </div>
        <figure>
        <img src="static/experiments/table1.png" alt="fail" width="90%"">
        <figcaption class="content has-text-left" style="word-break:normal">
          <i>Table 1. Base-to-novel generalization experiments of five baselines with and without our ATPrompt on 11 recognition datasets. HM:
          Harmonic Mean. ∆: HM improvement of ATPrompt over previous results. “ATPrompt” is abbreviated as “ATP”. Our method achieves
          consistent average performance improvement over different baselines.</i>
        </figcaption>
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Cross Dataset Experiments</strong>
        </div>
        <figure>
          <img src="static/experiments/table2.png" alt="fail" width="100%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 2. Cross-dataset generalization experiments of three baselines with and without our ATPrompt on 11 datasets. Our method achieves
            consistent average performance improvements over three baseline methods.</i> 
          </figcaption>
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Domain Generalization</strong>
        </div>
        <figure>
          <img src="static/experiments/table3.png" alt="fail" width="70%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 3. Domain generalization experiments of three baselines with and without our ATPrompt on 4 datasets. Our method achieves
            consistent average performance improvement over three baseline methods.</i>
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Comparison to Other Attributes</strong>
        </div>
        <p class="content has-text-left" style="text-indent:1em">
          Here we explore the effectiveness of attributes derived through alternative methods, specifically by manually selecting class-irrelevant and common attributes.
        </p>
        <figure>
          <img src="static/experiments/table6.png" alt="fail" width="50%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 4. Comparison of different attributes on Food101. The attributes obtained by our method achieve the best performance.</i> 
          </figcaption>
        </figure>
        <br>
        <p class="content has-text-left" style="text-indent:1em">
          The results indicate that manually selected irrelevant attributes exhibit comparable performance during training; however, they perform poorly when applied to new categories. This suggests that incorrect attribute tokens cause the soft tokens to develop biased representations, thereby diminishing their zero-shot generalization ability.
        </p>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Attribute Order</strong>
        </div>
        <p class="content has-text-left" style="text-indent:1em">
          In this study, we do not specifically focus on the order of attributes in ATPrompt because varying the sequence usually does not result in semantic deviations in reality. For example, phrases like “a yellow round
          leaf” and “a round yellow leaf” convey the same meaning.
        </p>
        <figure>
          <img src="static/experiments/table7.png" alt="fail" width="40%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 5. Comparison of different attribute orders on ImageNet.
            The order of attributes does not significantly affect the model, and performance fluctuations are within a reasonable range.</i> 
          </figcaption>
        </figure>
        <br>
        <p class="content has-text-left" style="text-indent:1em">
          From this table, we observe that despite variations in order, similar results are consistently produced, and the performance fluctuations across different orders remain within a reasonable range.
        </p>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Prompt Operation of Deep Version</strong>
        </div>
        <p class="content has-text-left" style="text-indent:1em">
          In ATPrompt-Deep, we exclusively drop class soft tokens while retaining both hard and soft attribute tokens after they pass through the block. 
          In the following table, we compare the performance of partial drop (i.e., removing attribute soft tokens while retaining hard tokens) and full drop (i.e., removing both attribute softand hard tokens) operations.
        </p>
        <figure>
          <img src="static/experiments/table4.png" alt="fail" width="50%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 6. Comparison of operations on deep soft and hard attribute
            tokens based on MaPLe+ATPrompt. Preserving hard and soft attribute tokens in deep layers performs better than other operations.</i>
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Attribute Bases and Searched Results</strong>
        </div>
        <figure>
          <img src="static/experiments/table5.png" alt="fail" width="80%"">
          <figcaption class="content has-text-centered" style="word-break:normal">
            <i>Table 7. Attribute bases and searched results for each dataset.</i>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is 3">Other Useful Materials</h2>
        <div class="content has-text-left" style="word-break:normal" >
          <p>
            1. If you are interested in prompt learning and want to know more about related work, we also maintain <a href="https://github.com/zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" target="_blank" style="color:rgb(31, 147, 255)">[a curated list of awesome prompt/adapter learning methods for VLMs]</a><img src="https://img.shields.io/github/stars/zhengli97/Awesome-Prompt-Learning-for-Vision-Language-Models?style=social"/> for your reference. 
            <br><br>
            2. In October 2024, I was invited by Jiangmen(将门) to give a talk about prompt learning methods. In this video <a href="https://www.techbeat.net/talk-info?id=915" target="_blank" style="color:rgb(31, 147, 255)">[Link]</a> , I introduce the motivation, principle and related work of the prompt learning method in detail. 
            If you can speak Chinese, this video might be a good material to help you quickly understand the field of prompt learning.
            <br><br>
            3. Before this work, I published a paper on prompt learning at CVPR-2024 called <a href="https://arxiv.org/abs/2403.02781" target="_blank" style="color:rgb(31, 147, 255)">PromptKD</a>. In this <a href="https://zhengli97.github.io/PromptKD/" target="_blank" style="color:rgb(31, 147, 255)">[project]</a> <img src="https://img.shields.io/github/stars/zhengli97/PromptKD?style=social"/>, I open-sourced the complete code and wrote a detailed <a href="https://zhuanlan.zhihu.com/p/684269963" target="_blank" style="color:rgb(31, 147, 255)">Chinese paper interpretation</a>.
            This interpretation is also a good learning material for your reference.
            <br><br>
            <strong>4. If you have any questions, please feel free to submit an <a href="https://github.com/zhengli97/ATPrompt/issues" target="_blank" style="color:rgb(31, 147, 255)">issue</a> on GitHub, or contact me by email (zhengli97[at]qq.com).</strong>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-left">
      <div class="column is-four-fifths">
    <h2 class="title">BibTeX (TBD)</h2>
    <p>
      If you find our paper is helpful for your research, please consider citing our paper.
    </p>
    <br>
    <pre><code>      
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container is-small">
    <div class="columns is-centered has-text-centered">
      <div class="column is-8">
        <div class="content is-small">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
