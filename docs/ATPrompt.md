# How to reproduce ATPrompt

## Preliminary

1. Create the environment and install Dassl.pytorch library. Please follow the instructions detailed in [[INSTALL.md](INSTALL.md)].

2. Prepare the dataset. Please follow the instructions detailed in [[DATASETS.md](DATASETS.md)]. For your download convenience, we have provided 14 datasets (excluding ImageNet-1K) in the Huggingface platform. [[HuggingFace_Download_Links](https://huggingface.co/zhengli97/prompt_learning_dataset)]

3. Download the original ViT-B/16 and ViT-L/14 CLIP model weights from the official OpenAI website. Then place these models in the `./clip` folder. Comment the `trainers/coop.py line 42` and uncomment the `line 43`.  
[[ViT-B/16 CLIP](https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt)] [[ViT-L/14 CLIP](https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt)]

## ğŸš€ Running ATPrompt

### Step I: Attribute Search (Optional)

**For more practical information about this process, please refer to [[Attribute_Search.md](Attribute_Search.md)].**

(1) Directly use our results.

Here we provide the five attribute bases obtained by querying the LLM (GPT-4o) and the final result after the differentiable attribute search. You can directly use our results for subsequent training.

Expand the list belowğŸ‘‡ to see the results:
<details>
<summary>Click to expand "Attribute Lists"</Summary>

| Dataset | Attribute Bases | Searched Results |
|:---------------:|:---------------:|:-----------------:|
| ImageNet-1K   | color, size, shape, habitat, behavior                  | (color, shape) |
| Caltech101    | shape, color, material, function, size                 | (shape,size) |
| Oxford Pets   | loyalty, affection, playfulness, energy, intelligence  | (playfulness, energy) |
| Stanford Cars | design, engine, performance, luxury, color             | (luxury) |
| Flowers-102   | color, flower, habitat, growth, season                 | (color, habitat, growth) |
| Food-101      | flavor, texture, origin, ingredients, preparation      | (flavor, preparation) |
| FGVC Aircraft | design, capacity, range, engines, liveries             | (design, range) |
| SUN-397       | architecture, environment, structure, design, function | (function) |
| DTD           | pattern, texture, color, design, structure             | (pattern, color, design) |
| EuroSAT       | habitat, foliage, infrastructure, terrain, watercourse | (habitat) |
| UCF-101       | precision, coordination, technique, strength, control  | (precision) |

Table 1. Attribute bases and searched results for each dataset.
</details>

<hr/>

(2) Reproduce the whole process on your own.

- Register a ChatGPT service account (We are using [ZhiZengZeng](https://gpt.zhizengzeng.com/#/)) and enter the API Key in `gpt_query.py line 27`. Then run the following code:   
```bash
python gpt_query.py
```    
In this way, you will get five output attributes after running the code.    
(You can change the input prompt in `gpt_query.py line 94` to specify as many attributes as you want.)   

- Enter the five attributes into the variables `ATT1_TEXT`, `ATT2_TEXT`, `ATT3_TEXT`, `ATT4_TEXT` and `ATT5_TEXT` in `scripts/attribute_compute/main.sh`. Then run the attribute search code:
```bash
sh scripts/attribute_compute/main.sh
```
Select the result with the **highest confidence** in the last epoch as our target attribute.

In the following **<Training Logs & Weights>**, we provide the complete attribute searching log on ten datasets for your reference.

<hr/>

### Step II: Prompt Learning with ATPrompt.

Here we take the **CoOp+ATPrompt** method as an example. You can switch to other baseline methods if you want.
(This implementation currently supports CoOp+ATPrompt, CoCoOp+ATPrompt, MaPLe+ATPrompt and DePT+ATPrompt methods.)

**(1) Base-to-Novel Experiments.**

1. The config files for each baseline method are provided in `configs/trainers/`. You can modify the hyperparameters in these config files.

2. Change the `DATA` in `scripts/coop/atprompt_base2new_train.sh line 4` to your current dataset path.

3. Run the following commands to train the model using the ATPrompt method:   

**ğŸš€ Training:**
```bash
# CoOp+ATPrompt, dataset=imagenet
sh scripts/coop/atprompt_base2new_train.sh imagenet

# CoOp+ATPrompt, dataset=caltech101
sh scripts/coop/atprompt_base2new_train.sh caltech101
```
**âš¡ Testing:**
```bash
# CoOp+ATPrompt, dataset=caltech101
sh scripts/coop/atprompt_base2new_test.sh caltech101
```

If you don't want to use ATPrompt, you can set `TRAINER.ATPROMPT.USE_ATPROMPT` in `scripts/coop/atprompt_base2new_train.sh line 31` to **False**.   
Or you can run the following command:

```bash
# Vanilla CoOp
sh scripts/coop/vanilla_base2new_train.sh imagenet
```

**(2) Cross-dataset & Domain Generalization Experiments.**

1. Change the `DATA` in `scripts/coop/xd_train.sh line 4` to your current dataset path.

2. Train the model on the source dataset (ImageNet) and select the best-performing model.

```bash
sh scripts/coop/atprompt_xd_train.sh
```

3. After training, evaluate the model on other recognition datasets. For example, the model trained with **seed 1** has the best performance.
So we evaluate its performance like this:

```bash
# Cross-dataset
# dataset=caltech101, seed=1
sh scripts/coop/atprompt_xd_eval.sh caltech101 1

# Domain Generalization
# dataset=imagenet_a, seed=1
sh scripts/coop/atprompt_xd_eval.sh imagenet_a 1
```

In the following part, we provide the complete training log and model weights of **CoOp+ATPrompt** for your reference.


<!-- ### ğŸ”¬ Experimental Results

The results are averaged over 3 seeds. Note that due to the limited number of training samples and network parameters, the performance results may fluctuate. If you cannot achieve the reported results, please run more experiments with different seeds. -->

<!-- #### Base-to-Novel Generalization

<details>
<summary>Click to expand "Result Figures".</Summary>
<figure>
<img src="images/exp_results.png" alt="fail" width="100%"">    
<figcaption class="content has-text-left" style="word-break:normal">Table 1: Base-to-novel generalization experiments of five baselines with and without our ATPrompt on 11 recognition datasets.
</figure>
</details> -->

## ğŸ“„ Training Logs & Weights

- Attribute Search.  
We provide the complete attribute searching log on ten datasets for your reference.   
[[Github Release](https://github.com/zhengli97/ATPrompt/releases/tag/training-log)]

- Base-to-Novel Generalization (CoOp+ATPrompt).   
We provide the complete training logs and model weights on 11 datasets for your reference.  
[[Github Release](https://github.com/zhengli97/ATPrompt/releases/tag/traininglog_and_weights)]

- Cross-dataset Prompt Learning (CoOp+ATPrompt).  
We provide model weights and training logs trained on the source dataset (ImageNet) under cross-dataset settings.  
[[Github Release](https://github.com/zhengli97/ATPrompt/releases/tag/weights)]

## Detailed Hyperparameters for Reproducing

In this part, we provide implementation details and hyperparameter settings for reproducing CoOp+ATPrompt, CoCoOp+ATPrompt, MaPLe+ATPrompt and DePT+ATPrompt. 

**ğŸ’¡ Important Note: Reproduction with the following settings may deviate or fluctuate from the reported values. This is due to the randomness of the training data partitioning (`oxford_pets.py line 77`). This is normal. We recommend that researchers run more experiments with different seeds to reproduce the corresponding results stably.**

Below is the attribute table used for different datasets:

| Datasets | Attributes |
| :--: | :--: |
| ImageNet | color, shape |
| Caltech101 | shape, size |
| OxfordPets | playfulness, energy |
| Stanford Cars | luxury |
| Flowers102 | color, habitat, growth |
| Food101 | flavor, preparation |
| FGVC Aircraft | design, range |
| SUN 397 | function |
| DTD | pattern, color, design |
| EuroSAT | habitat |
| UCF101 | precision |

The above attributes correspond to the `cfg.TRAINER.ATPROMPT.ATT1_TEXT`, `cfg.TRAINER.ATPROMPT.ATT2_TEXT`, and `cfg.TRAINER.ATPROMPT.ATT3_TEXT` variables in the code.

If you want to experiment with other attribute words, you can change the variable values â€‹â€‹in the function defined in `train.py line 154`.

### Base-to-Novel Experiments

### CoOp+ATPrompt

In this experiment, keep other hyperparameters unchanged. For datasets including Caltech, OxfordPets, StanfordCars, Flowers, Food101, Aircraft, SUN397, EuroSAT, and UCF101, we specifically set EPO=100, NCTX=2.

For the DTD dataset, set EPO=100, NCTX=4.

For the ImageNet dataset, set EPO=10, NCTX=2.

### CoCoOp+ATPrompt

In this experiment, keep other parameters unchanged. For datasets including ImageNet, Caltech, OxfordPets, Food101, FGVC Aircraft, SUN397, and DTD, we specifically set EPO=10, NCTX=2.

For the UCF-101 dataset, set EPO=10, NCTX=4.

For StanfordCars, Flowers, and EuroSAT datasets, set EPO=10, NCTX=6.

### MaPLe+ATPrompt

In this experiment, keep other parameters unchanged. For datasets including Caltech101, OxfordPets, Flowers, and EuroSAT, we specifically set EPO=10, NCTX=4.

For datasets including ImageNet, StanfordCars, Food101, SUN397, DTD, set EPO=5, NCTX=4.

For the FGVC Aircraft and UCF101 datasets, set EPO=5, NCTX=2.

### DePT+ATPrompt

Note: There is a loss balance hyperparameter w in DePT, which is generally set to 0.7 by default.

In this experiment, other parameters are kept unchanged. For the datasets including Caltech, DTD, EuroSAT, FGVC Aircraft, Food101, Flowers, SUN397, and UCF101, we set EPO=10, NCTX=4, where w=0.6 is set for Caltech and DTD datasets, and w=0.5 is set for UCF101.

For the StanfordCars and OxfordPets datasets, we set EPO=10, NCTX=2, where w=0.6 is set for StanfordCars.

### Cross-dataset & Domain Generalization Experiments

In this experimental setting, the usual practice is to select a model trained on the source dataset and measure its generalization performance on other datasets. There will be performance fluctuations when reproducing it yourself, so we recommend that researchers run several seeds and select the best-performing model for evaluation.

### CoOp+ATPrompt

In this experiment, other parameters are kept unchanged. On the ImageNet-1K source dataset, we set NCTX=4 and EPO=10.

### CoCoOp+ATPrompt

In this experiment, other parameters are kept unchanged. On the ImageNet-1K source dataset, we set NCTX=4 and EPO=5.

### MaPLe+ATPrompt

In this experiment, other parameters are kept unchanged. On the ImageNet-1K source dataset, we set NCTX=4 and EPO=2.

## è®ºæ–‡ç»“æœå¤ç°æŒ‡å¼• (ä¸­æ–‡ç‰ˆ)

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¼šæä¾›ç”¨äºå¤ç°CoOp+ATPromptï¼ŒCoCoOp+ATPrompt, MaPLe+ATPromptçš„å®ç°ç»†èŠ‚å’Œè¶…å‚æ•°è®¾å®šã€‚(DePTå’ŒPromptKDçš„å®éªŒå› ä¸ºä»£ç è¿˜æ²¡æ•´åˆè¿›æ¥ï¼Œæš‚æ—¶æ²¡æœ‰æä¾›ï¼ŒDePTå’ŒPromptKDç”±äºæ˜¯ç”¨çš„ä¸ä¸€æ ·çš„å®ç°æ¡†æ¶ï¼Œæ‰€ä»¥éœ€è¦ä¸€å®šçš„æ—¶é—´è¿›è¡Œè¿ç§»)ã€‚

**ğŸ’¡ é‡è¦æç¤ºï¼šæŒ‰ç…§ä»¥ä¸‹è®¾å®šè¿›è¡Œå¤ç°å¯èƒ½ä¼šä¸æŠ¥å‘Šæ•°å€¼å­˜åœ¨åå·®æˆ–è€…æ³¢åŠ¨ï¼Œè¿™æ˜¯ç”±äºåˆ’åˆ†è®­ç»ƒæ•°æ®æ—¶çš„éšæœºæ€§å¼•èµ·çš„(`datasets/oxford_pets.py line 77`)ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚æˆ‘ä»¬æ¨èç ”ç©¶è€…å¤šè·‘ä¸€äº›ä¸åŒç§å­çš„å®éªŒä»¥ç¨³å®šå¤ç°å¯¹åº”çš„ç»“æœã€‚**

ä»¥ä¸‹æ˜¯å¯¹äºä¸åŒæ•°æ®é›†æ‰€ç”¨çš„å±æ€§è¡¨ï¼š

| Datasets | Attributes |
| :--: | :--: |
| ImageNet | color, shape |
| Caltech101 | shape, size |
| OxfordPets | playfulness, energy |
| Stanford Cars | luxury |
| Flowers102 | color, habitat, growth |
| Food101 | flavor, preparation |
| FGVC Aircraft | design, range |
| SUN 397 | function |
| DTD | pattern, color, design |
| EuroSAT | habitat |
| UCF101 | precision |

è¿™äº›å±æ€§åˆ†åˆ«å¯¹åº”ä»£ç ä¸­çš„`cfg.TRAINER.ATPROMPT.ATT1_TEXT`, `cfg.TRAINER.ATPROMPT.ATT2_TEXT`, `cfg.TRAINER.ATPROMPT.ATT3_TEXT`å˜é‡ã€‚

å¦‚æœä½ æƒ³è¦å°è¯•å…¶ä»–çš„å±æ€§è¯ï¼Œå¯ä»¥æ›´æ”¹`train.py line 154`æ‰€å®šä¹‰å‡½æ•°é‡Œçš„å˜é‡å€¼ã€‚

### Base-to-Novelå®éªŒ

### CoOp+ATPrompt

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–è¶…å‚æ•°ä¸å˜ï¼Œå¯¹äºCaltech, OxfordPets, StanfordCars, Flowers, Food101, Aircraft, SUN397, EuroSAT, UCF101ï¼Œæˆ‘ä»¬ç‰¹åˆ«åœ°è®¾å®šEPO=100, NCTX=2ã€‚

å¯¹äºDTD, è®¾å®šEPO=100, NCTX=4ã€‚

å¯¹äºImageNet, è®¾å®šEPO=10, NCTX=2ã€‚

### CoCoOp+ATPrompt

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼Œå¯¹äºImageNet, Caltech, OxfordPets, Food101, FGVC Aircraft, SUN397, DTD è®¾å®šEPO=10, NCTX=2ã€‚

å¯¹äºUCF-101, è®¾å®šEPO=10, NCTX=4ã€‚

å¯¹äºStanfordCars, Flowers, EuroSAT, è®¾å®šEPO=10, NCTX=6ã€‚

### MaPLe+ATPrompt

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼Œå¯¹äºCaltech101, OxfordPets, Flowers, EuroSAT, è®¾å®šEPO=10, NCTX=4ã€‚

å¯¹äºImageNet, StanfordCars, Food101, SUN397, DTD, è®¾å®šEPO=5, NCTX=4ã€‚

å¯¹äºFGVC Aircraft, UCF101, è®¾å®šEPO=5, NCTX=2ã€‚

### DePT+ATPrompt

æ³¨ï¼šDePTä¸­æœ‰ä¸€ä¸ªæŸå¤±å¹³è¡¡è¶…å‚æ•°wï¼Œä¸€èˆ¬é»˜è®¤è®¾å®šä¸º0.7ã€‚

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼Œå¯¹äºCaltech, DTD, EuroSAT, FGVC Aircraft, Food101, Flowers, SUN397, UCF101 è®¾å®šEPO=10, NCTX=4, w=0.7 å…¶ä¸­å¯¹äºCaltech, DTD, è®¾å®šEPO=10, NCTX=4, w=0.6ï¼Œ å¯¹äºUCF101, è®¾å®šEPO=10, NCTX=4, w=0.5ã€‚

å¯¹äºOxfordPets, è®¾å®šEPO=10, NCTX=2, w=0.7, å¯¹äºStanfordCarsï¼Œè®¾å®šEPO=10, NCTX=2, w=0.6ã€‚

### Cross-dataset & Domain Generalization å®éªŒ

åœ¨è¯¥å®éªŒè®¾å®šä¸‹ï¼Œå› ä¸ºæ˜¯é€‰æ‹©ä¸€ä¸ªåœ¨æºåŸŸä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å…¶ä»–æ•°æ®é›†ä¸Šæµ‹æ³›åŒ–ï¼Œåœ¨è‡ªå·±å¤ç°çš„æ—¶å€™ä¼šå­˜åœ¨æ€§èƒ½çš„æ³¢åŠ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å»ºè®®ç ”ç©¶è€…å¤šè·‘å‡ ä¸ªseedï¼Œé€‰æ‹©å…¶ä¸­è¾ƒå¥½çš„æ¨¡å‹è¿›è¡ŒéªŒè¯è¯„ä¼°ã€‚

### CoOp+ATPrompt

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼Œåœ¨ImageNet-1Kæºæ•°æ®é›†ä¸Šï¼Œè®¾å®šNCTX=4, EPO=10ã€‚

### CoCoOp+ATPrompt

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼Œåœ¨ImageNet-1Kæºæ•°æ®é›†ä¸Šï¼Œè®¾å®šNCTX=4, EPO=5ã€‚

### MaPLe+ATPrompt

åœ¨è¯¥å®éªŒä¸­ï¼Œä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼Œåœ¨ImageNet-1Kæºæ•°æ®é›†ä¸Šï¼Œè®¾å®šNCTX=4, EPO=2ã€‚
